{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier (Using Keras and Tensorflow backend)\n",
    "\n",
    "### Convolutional Neural Network\n",
    "\n",
    "Keras runs on top of TensorFlow. It makes building models intuitive, since we can define each layer as it's own line of code.\n",
    "```\n",
    "cat ~/.keras/keras.json \n",
    "{\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"floatx\": \"float32\",\n",
    "    \"image_dim_ordering\": \"tf\", \n",
    "    \"image_data_format\": \"channels_last\",\n",
    "    \"backend\": \"tensorflow\"\n",
    "}\n",
    "```\n",
    "### This example uses the CPU which is much slow compared to GPU (other example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##This notebook is built around using tensorflow as the backend for keras\n",
    "# !pip install pillow\n",
    "# !KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np           # math\n",
    "# from parser import load_data # data loading\n",
    "from keras.models import Sequential # machine learning loads ~/.keras/keras.json\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = '/home/ricardo/Downloads/kaggle/train'\n",
    "validation_data_dir = '/home/ricardo/Downloads/kaggle/test1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23778 images belonging to 2 classes.\n",
      "Found 1222 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Convolutional Net\n",
    "\n",
    "## Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 30\n",
    "nb_train_samples = 128\n",
    "nb_validation_samples = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "128/128 [==============================] - 151s - loss: 0.7454 - acc: 0.5386 - val_loss: 0.6781 - val_acc: 0.5463\n",
      "Epoch 2/30\n",
      "128/128 [==============================] - 142s - loss: 0.6685 - acc: 0.6089 - val_loss: 0.6297 - val_acc: 0.6239\n",
      "Epoch 3/30\n",
      "128/128 [==============================] - 142s - loss: 0.6301 - acc: 0.6562 - val_loss: 0.5699 - val_acc: 0.7066\n",
      "Epoch 4/30\n",
      "128/128 [==============================] - 153s - loss: 0.6083 - acc: 0.6636 - val_loss: 0.5764 - val_acc: 0.6844\n",
      "Epoch 5/30\n",
      "128/128 [==============================] - 145s - loss: 0.6048 - acc: 0.6904 - val_loss: 0.5525 - val_acc: 0.7369\n",
      "Epoch 6/30\n",
      "128/128 [==============================] - 146s - loss: 0.5620 - acc: 0.7256 - val_loss: 0.5628 - val_acc: 0.7106\n",
      "Epoch 7/30\n",
      "128/128 [==============================] - 141s - loss: 0.5760 - acc: 0.7114 - val_loss: 0.5118 - val_acc: 0.7586\n",
      "Epoch 8/30\n",
      "128/128 [==============================] - 151s - loss: 0.5585 - acc: 0.7192 - val_loss: 0.5307 - val_acc: 0.7315\n",
      "Epoch 9/30\n",
      "128/128 [==============================] - 161s - loss: 0.5587 - acc: 0.7310 - val_loss: 0.4969 - val_acc: 0.7680\n",
      "Epoch 10/30\n",
      "128/128 [==============================] - 147s - loss: 0.5372 - acc: 0.7383 - val_loss: 0.4747 - val_acc: 0.7751\n",
      "Epoch 11/30\n",
      "128/128 [==============================] - 159s - loss: 0.5408 - acc: 0.7397 - val_loss: 0.4740 - val_acc: 0.7825\n",
      "Epoch 12/30\n",
      "128/128 [==============================] - 152s - loss: 0.5227 - acc: 0.7456 - val_loss: 0.4824 - val_acc: 0.7615\n",
      "Epoch 13/30\n",
      "128/128 [==============================] - 160s - loss: 0.5131 - acc: 0.7568 - val_loss: 0.5142 - val_acc: 0.7539\n",
      "Epoch 14/30\n",
      "128/128 [==============================] - 175s - loss: 0.4996 - acc: 0.7856 - val_loss: 0.4584 - val_acc: 0.7837\n",
      "Epoch 15/30\n",
      "128/128 [==============================] - 178s - loss: 0.5280 - acc: 0.7588 - val_loss: 0.4732 - val_acc: 0.7738\n",
      "Epoch 16/30\n",
      "128/128 [==============================] - 175s - loss: 0.5021 - acc: 0.7642 - val_loss: 0.4399 - val_acc: 0.8021\n",
      "Epoch 17/30\n",
      "128/128 [==============================] - 172s - loss: 0.4961 - acc: 0.7720 - val_loss: 0.4442 - val_acc: 0.8006\n",
      "Epoch 18/30\n",
      "128/128 [==============================] - 182s - loss: 0.5141 - acc: 0.7749 - val_loss: 0.4926 - val_acc: 0.7690\n",
      "Epoch 19/30\n",
      "128/128 [==============================] - 166s - loss: 0.4877 - acc: 0.7803 - val_loss: 0.4810 - val_acc: 0.7710\n",
      "Epoch 20/30\n",
      "128/128 [==============================] - 177s - loss: 0.4919 - acc: 0.7690 - val_loss: 0.5438 - val_acc: 0.7708\n",
      "Epoch 21/30\n",
      "128/128 [==============================] - 168s - loss: 0.5203 - acc: 0.7563 - val_loss: 0.5304 - val_acc: 0.7485\n",
      "Epoch 22/30\n",
      "128/128 [==============================] - 166s - loss: 0.4894 - acc: 0.7783 - val_loss: 0.4622 - val_acc: 0.7977\n",
      "Epoch 23/30\n",
      "128/128 [==============================] - 163s - loss: 0.4898 - acc: 0.7822 - val_loss: 0.4884 - val_acc: 0.7668\n",
      "Epoch 24/30\n",
      "128/128 [==============================] - 168s - loss: 0.4693 - acc: 0.7886 - val_loss: 0.4487 - val_acc: 0.7922\n",
      "Epoch 25/30\n",
      "128/128 [==============================] - 156s - loss: 0.4743 - acc: 0.7827 - val_loss: 0.4539 - val_acc: 0.7740\n",
      "Epoch 26/30\n",
      "128/128 [==============================] - 162s - loss: 0.4562 - acc: 0.7988 - val_loss: 0.4474 - val_acc: 0.7877\n",
      "Epoch 27/30\n",
      "128/128 [==============================] - 175s - loss: 0.4486 - acc: 0.8081 - val_loss: 0.4203 - val_acc: 0.8143\n",
      "Epoch 28/30\n",
      "128/128 [==============================] - 168s - loss: 0.4786 - acc: 0.7793 - val_loss: 0.4341 - val_acc: 0.8250\n",
      "Epoch 29/30\n",
      "128/128 [==============================] - 176s - loss: 0.4929 - acc: 0.7852 - val_loss: 0.4535 - val_acc: 0.7949\n",
      "Epoch 30/30\n",
      "128/128 [==============================] - 174s - loss: 0.4435 - acc: 0.8052 - val_loss: 0.4284 - val_acc: 0.8031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6bc3590>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        steps_per_epoch=nb_train_samples,\n",
    "        epochs=nb_epoch,\n",
    "        validation_steps=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/basic_cnn_20_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights('models_trained/basic_cnn_20_epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on validation set\n",
    "\n",
    "Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42391207523462249, 0.80388252856186693]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After ~30 epochs on the CPU the neural network reached ~80% accuracy. But again we can improve it.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Slides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
